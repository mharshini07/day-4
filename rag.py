# -*- coding: utf-8 -*-
"""rag.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cD6MmRD9QnmYB7xN5xn-_6JHd7kk9Klt
"""

import pdfplumber

# Extract all text from PDF
pdf_text = ""
with pdfplumber.open("Case.pdf") as pdf:
    for page in pdf.pages:
        page_text = page.extract_text()
        if page_text:         # Handle blank pages
            pdf_text += page_text + "\n\n"

# Split into chunks by double newlines (paragraphs)
chunks = [chunk.strip() for chunk in pdf_text.split("\n\n") if chunk.strip()]
print(f"Total chunks: {len(chunks)}")
print("Sample chunk:", chunks[0])
from sentence_transformers import SentenceTransformer
import numpy as np

model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = model.encode(chunks)  # shape: (num_chunks, embedding_dim)
if isinstance(embeddings, np.ndarray):
    embeddings = embeddings.tolist()
import chromadb
from chromadb.config import Settings

client = chromadb.PersistentClient(path="./chroma_db")
collection = client.get_or_create_collection("legal_case_law")

ids = [f"chunk_{i}" for i in range(len(chunks))]
metadatas = [{"source": "Case.pdf", "chunk_id": i} for i in range(len(chunks))]

collection.add(
    ids=ids,
    documents=chunks,
    embeddings=embeddings,
    metadatas=metadatas
)
user_query = "What Supreme Court directives relate to food labeling?"
query_embedding = model.encode([user_query])[0].tolist()  # get list not ndarray

results = collection.query(
    query_embeddings=[query_embedding],
    n_results=2,
    include=["documents", "metadatas", "distances"]
)

# Take the first list returned for each field (since only one query)
returned_docs = results["documents"][0]
returned_metas = results["metadatas"][0]
returned_dists = results["distances"][0]

for doc, meta, dist in zip(returned_docs, returned_metas, returned_dists):
    print(f"Found chunk: {meta['chunk_id']} (distance={dist:.4f})\n{doc}\n")